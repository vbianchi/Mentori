# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables
#
# INSTRUCTIONS:
# 1. Copy this file to a new file named .env (cp .env.example .env)
# 2. Fill in the required values (e.g., GOOGLE_API_KEY).
# 3. The .env file is included in .gitignore to protect your secrets.
#
# This file serves as a complete blueprint for the project's configuration.
# -----------------------------------------------------------------------------

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=""

# Get your Tavily API Key from https://tavily.com (for the search tool)
TAVILY_API_KEY=""

# Required for the PubMed search tool.
# See: https://www.ncbi.nlm.nih.gov/books/NBK25497/
ENTREZ_EMAIL=""


# --- Available LLM Models for UI Selection ---
# Comma-separated list of Gemini models accessible with your API key.
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-05-06,gemini-2.0-flash,gemini-2.0-flash-lite,gemini-1.5-flash

# Comma-separated list of Ollama models you have pulled locally.
# Ensure 'ollama serve' is running and you have pulled these models.
OLLAMA_AVAILABLE_MODELS=mistral:latest,llama3.2:latest,llama4:scout,deepseek-r1:8b,qwen3:4b,qwen3:8b,qwen3:30b-a3b,gemma3:12b,gemma3:latest


# Base URL for your local Ollama instance.
OLLAMA_BASE_URL=http://localhost:11434


# --- Core LLM Configuration ---
# Default LLM to use if a specific role's LLM is not set or fails.
# Format: "provider::model_name" (e.g., "gemini::gemini-1.5-flash", "ollama::llama3")
DEFAULT_LLM_ID=gemini::gemini-2.0-flash


# --- Role-Specific LLM Configuration (Development Defaults) ---
# For initial development (Phases 1-3), all roles are defaulted to a single, fast model.
# As we implement and tune each component in Phase 4, these can be updated to more
# specialized models as commented in the recommendations.

# For classifying user intent (PLAN vs. DIRECT_QA). Needs to be fast.
# Recommended: gemini::gemini-1.5-flash, ollama::mistral
INTENT_CLASSIFIER_LLM_ID=gemini::gemini-2.0-flash

# For generating the initial multi-step plan. Needs strong reasoning.
# Recommended: gemini::gemini-1.5-pro, ollama::llama3
PLANNER_LLM_ID=gemini::gemini-2.0-flash

# For validating each plan step and formulating tool inputs. Needs good instruction following.
# Recommended: gemini::gemini-1.5-flash, ollama::codellama
CONTROLLER_LLM_ID=gemini::gemini-2.0-flash

# Default workhorse LLM for the Executor (ReAct agent) if not overridden by the UI.
# Recommended: gemini::gemini-1.5-flash, ollama::llama3
EXECUTOR_DEFAULT_LLM_ID=gemini::gemini-2.0-flash

# For evaluating the outcome of an executed plan. Needs strong analytical skills.
# Recommended: gemini::gemini-1.5-pro, ollama::llama3
EVALUATOR_LLM_ID=gemini::gemini-2.0-flash


# --- Agent & LLM Tuning ---
# Max iterations for the agent executor loop for a single step.
AGENT_MAX_ITERATIONS=15
# Conversation memory window size (number of past user/AI message pairs to keep).
AGENT_MEMORY_WINDOW_K=10

# Default model temperature (0.0 to 1.0). Higher is more creative.
GEMINI_TEMPERATURE=0.5
OLLAMA_TEMPERATURE=0.5


# --- Tool Specific Settings ---
# Timeout in seconds for shell commands (e.g., running python, Rscript, git).
TOOL_SHELL_TIMEOUT=120
# Max characters of output to return from a shell command.
TOOL_SHELL_MAX_OUTPUT=5000

# Timeout in seconds for installing packages (e.g., pip, uv).
TOOL_INSTALLER_TIMEOUT=300


# --- Server Settings ---
# The backend server will run on this port.
# Ensure this port is free or change it.
BACKEND_PORT=8765

# The file server for serving agent-created artifacts will run on this port.
FILE_SERVER_PORT=8766

# Hostname to use when constructing file URLs for the client.
# For Docker, 'localhost' works if ports are mapped to the host.
# If running on a remote server, this should be the server's public IP/domain.
FILE_SERVER_HOSTNAME=localhost

# Max WebSocket message size in bytes (e.g., 16MB = 16 * 1024 * 1024).
WEBSOCKET_MAX_SIZE_BYTES=16777216


# --- Optional Settings ---
# Logging level for the backend (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO