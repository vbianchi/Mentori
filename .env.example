# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables
#
# INSTRUCTIONS:
# 1. Copy this file to a new file named .env (cp .env.example .env)
# 2. Fill in the required values (e.g., GOOGLE_API_KEY).
# 3. The .env file is included in .gitignore to protect your secrets.
#
# This file serves as a complete blueprint for the project's configuration.
# -----------------------------------------------------------------------------

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIz..."

# Get your Tavily API Key from https://tavily.com (for the search tool)
TAVILY_API_KEY="tvly-..."

# --- Model Configuration (Optional) ---
# This section defines the LLMs used by each agent role.
# The format is "provider::model_name".
# Supported providers: "gemini", "ollama".

# -- Available Models --
# Comma-separated list of models the UI should display in the dropdowns.
GEMINI_AVAILABLE_MODELS=gemini-1.5-pro-latest,gemini-1.5-flash-latest
OLLAMA_AVAILABLE_MODELS=llama3,codellama

# -- Default Models for Each Agent Role --
# If a specific variable is not set, the UI will use the global DEFAULT_LLM_ID.
DEFAULT_LLM_ID="gemini::gemini-1.5-flash-latest"

# The Router: Classifies the user's initial request.
ROUTER_LLM_ID="gemini::gemini-1.5-flash-latest"

# The Chief Architect: Creates the structured plan.
CHIEF_ARCHITECT_LLM_ID="gemini::gemini-1.5-pro-latest"

# The Site Foreman: Prepares each tool call for both simple and complex tracks.
SITE_FOREMAN_LLM_ID="gemini::gemini-1.5-flash-latest"

# The Project Supervisor: Evaluates the outcome of each step.
PROJECT_SUPERVISOR_LLM_ID="gemini::gemini-1.5-flash-latest"

# The Editor: Synthesizes final answers, updates memory, and summarizes history.
EDITOR_LLM_ID="gemini::gemini-1.5-pro-latest"


# --- Server Configuration (Optional) ---
# The host and port for the backend WebSocket server.
BACKEND_HOST="0.0.0.0"
BACKEND_PORT="8765"

# The host and port for the HTTP file server.
FILE_SERVER_PORT="8766"

# The base URL for a local Ollama server, if used.
# 'host.docker.internal' lets the Docker container talk to your host machine.
OLLAMA_BASE_URL="http://host.docker.internal:11434"

# Set the logging level for the backend. (e.g., DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL="INFO"