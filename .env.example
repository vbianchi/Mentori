# --- Required Settings ---

# Google API Key for Gemini models
# Get one from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIz..." # <-- REPLACE with your actual Google API Key

# Email for NCBI Entrez (Required for PubMed Tool)
# Used by NCBI to identify requests, not for login. Replace with your real email.
ENTREZ_EMAIL="your.email@example.com" # <-- REPLACE with your actual email


# --- Core LLM Configuration ---

# Default LLM Identifier to use on startup or if none selected in UI.
# Format: "provider::model_name" (e.g., "gemini::gemini-1.5-flash", "ollama::llama3:latest")
# Ensure this model ID is derived from the available models listed below.
DEFAULT_LLM_ID="gemini::gemini-1.5-flash" # <-- ADJUST Default if needed

# Base URL for your Ollama instance (only needed if using Ollama models)
# If Ollama is running in WSL/Linux and backend is in Docker (host network mode), use http://localhost:11434
# If Ollama is on a different machine, use its IP address/hostname.
OLLAMA_BASE_URL=http://localhost:11434


# --- LLM Model Selection Configuration (for UI Dropdown) ---

# Comma-separated list of available Gemini models for the UI dropdown
# Ensure these models are accessible with your GOOGLE_API_KEY
# Example: GEMINI_AVAILABLE_MODELS=gemini-1.5-flash,gemini-1.5-pro-latest
GEMINI_AVAILABLE_MODELS=gemini-1.5-flash,gemini-1.5-pro-latest

# Comma-separated list of available Ollama models for the UI dropdown
# Ensure these models are pulled and available in your Ollama instance (use 'ollama list' to check)
# Example: OLLAMA_AVAILABLE_MODELS=gemma:2b,llama3:latest,mistral:latest
OLLAMA_AVAILABLE_MODELS=gemma:2b,llama3:latest


# --- Agent & LLM Tuning ---

# Maximum number of steps (LLM calls + Tool uses) the agent can take per turn.
AGENT_MAX_ITERATIONS=15

# Number of past messages (user + agent pairs) to keep in the agent's short-term memory.
AGENT_MEMORY_WINDOW_K=10

# Controls randomness/creativity for Gemini models (0.0 to 1.0). Higher is more creative.
GEMINI_TEMPERATURE=0.7

# Controls randomness/creativity for Ollama models (0.0 to 1.0). Higher is more creative.
OLLAMA_TEMPERATURE=0.5


# --- Tool Settings ---

# Max characters to extract from a web page using web_page_reader.
TOOL_WEB_READER_MAX_LENGTH=4000
# Timeout in seconds for fetching web pages.
TOOL_WEB_READER_TIMEOUT=15

# Timeout in seconds for shell commands run via workspace_shell tool.
TOOL_SHELL_TIMEOUT=60
# Max characters of shell command output to return to the agent.
TOOL_SHELL_MAX_OUTPUT=3000

# Timeout in seconds for installing packages via python_package_installer tool.
TOOL_INSTALLER_TIMEOUT=300

# Default max results for pubmed_search tool (user can override in query).
TOOL_PUBMED_DEFAULT_MAX_RESULTS=5
# Max characters for the abstract snippet returned by pubmed_search tool.
TOOL_PUBMED_MAX_SNIPPET=250

# Character length above which a warning is appended to PDF tool output.
# Does NOT truncate the output, just adds a warning.
TOOL_PDF_READER_WARNING_LENGTH=20000 # <-- NEW


# --- Server Settings ---

# Max size in bytes for a single WebSocket message. (Default: 16777216 = 16MB)
WEBSOCKET_MAX_SIZE_BYTES=16777216
# Interval in seconds for sending WebSocket pings to keep connection alive.
WEBSOCKET_PING_INTERVAL=20
# Timeout in seconds for waiting for a WebSocket pong response.
WEBSOCKET_PING_TIMEOUT=30

# Timeout in seconds for direct shell commands (not used by agent tools).
DIRECT_COMMAND_TIMEOUT=120


# --- Optional Settings ---

# Hostname used when constructing artifact URLs for the client browser to access.
# Defaults to 'localhost' if not set. Usually 'localhost' is correct when running locally.
# FILE_SERVER_HOSTNAME=localhost

# Logging level for the backend server
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL (Defaults to INFO if not set)
# LOG_LEVEL=INFO

