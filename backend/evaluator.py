# backend/evaluator.py
import logging
from typing import List, Dict, Any, Tuple, Optional

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field, conlist
from langchain_core.tools import BaseTool
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.base import BaseCallbackHandler


from backend.config import settings
from backend.llm_setup import get_llm
from backend.planner import PlanStep
from backend.callbacks import LOG_SOURCE_EVALUATOR_STEP, LOG_SOURCE_EVALUATOR_OVERALL


logger = logging.getLogger(__name__)

class StepCorrectionOutcome(BaseModel):
    step_achieved_goal: bool = Field(description="Boolean indicating if the step's primary goal was achieved based on the executor's output.")
    assessment_of_step: str = Field(description="Detailed assessment of the step's outcome, explaining why it succeeded or failed. If failed, explain the discrepancy between expected and actual outcome.")
    is_recoverable_via_retry: Optional[bool] = Field(description="If step_achieved_goal is false, boolean indicating if the step might be recoverable with a retry using a modified approach. Null if goal achieved.", default=None)
    suggested_new_tool_for_retry: Optional[str] = Field(description="If recoverable, the suggested tool name for the retry attempt (can be 'None'). Null if not recoverable or goal achieved.", default=None)
    # --- MODIFIED: Enhanced retry instructions guidance ---
    suggested_new_input_instructions_for_retry: Optional[str] = Field(
        description="If recoverable, new or revised input instructions for the Controller to formulate the tool_input for the retry. Null if not recoverable or goal achieved. **Crucially, if the failure was because the Executor described its action or output instead of providing the direct content, these instructions MUST explicitly guide the Controller to instruct the Executor to output ONLY the raw content itself in its 'Final Answer'.**"
    )
    confidence_in_correction: Optional[float] = Field(description="If recoverable, confidence (0.0-1.0) in the suggested correction. Null if not recoverable or goal achieved.", default=None)

# --- MODIFIED: STEP_EVALUATOR_SYSTEM_PROMPT_TEMPLATE to be more forceful on retry ---
STEP_EVALUATOR_SYSTEM_PROMPT_TEMPLATE = """You are an expert Step Evaluator for a research agent. Your task is to meticulously assess if a single executed plan step achieved its intended goal, based on its output and the original expectations.
**Context for Evaluation:**
- Original User Query: {original_user_query}
- Current Plan Step Being Evaluated:
    - Description: {current_step_description}
    - Expected Outcome (from Planner): {current_step_expected_outcome}
- Controller's Decision for this attempt:
    - Tool Used by Executor: {controller_tool_used}
    - Formulated Input for Executor/Tool: {controller_tool_input}
- Actual Output from Executor/Tool for this attempt (`step_executor_output`):
  ---
  {step_executor_output}
  ---

**Your Evaluation Task:**
1.  Determine `step_achieved_goal` (True/False): Did the `step_executor_output` successfully fulfill the `current_step_expected_outcome`? Be strict but fair.

    * **Content Expectation Check (CRITICAL):**
        * If the `current_step_expected_outcome` implies the generation or availability of specific *content* (e.g., "A comprehensive research report...", "The text of a poem...", "Python code to perform X..."), then `step_achieved_goal` is True **ONLY IF the `step_executor_output` IS that actual content (or a substantial, directly usable part of it).**
        * A message *about* the content (e.g., "The report is generated and available," "The poem has been written," "Code is ready") is **NOT sufficient** for `step_achieved_goal` to be True if the `expected_outcome` was the content itself. The `step_executor_output` must *be* the report, poem, code, etc.
        * If `step_executor_output` is merely a descriptive message confirming an action, but the `expected_outcome` clearly implies the *resultant content* should be present, then `step_achieved_goal` MUST be False.
    * **Format Handling:**
        * If the `expected_outcome` is described as textual content that might be generated by an LLM (e.g., "Python script content", "JSON string", "Markdown report"), the `step_executor_output` might be enclosed in Markdown code fences (e.g., ```python ... ``` or ```json ... ``` or ```markdown ... ```).
        * In such cases, you should **evaluate the content *within* the fences**. The presence of the fences themselves does not mean the goal wasn't achieved if the inner content matches the expectation.
2.  Provide a detailed `assessment_of_step`: Explain your reasoning for True/False. If False, clearly state what went wrong, what was missing, or how the `step_executor_output` failed to meet the true intent of the `current_step_expected_outcome` (especially regarding content vs. description of content).
3.  If `step_achieved_goal` is False:
    a. Determine `is_recoverable_via_retry` (True/False): Could a different tool, different tool input, or a slightly modified approach likely succeed on a retry? Consider if the error was transient, a misunderstanding, or a fundamental flaw. Max retries are limited.
    b. If `is_recoverable_via_retry` is True:
        i. `suggested_new_tool_for_retry`: From the available tools ({available_tools_summary}), suggest the best tool for a retry (can be 'None' if the LLM should try again directly).
        ii. `suggested_new_input_instructions_for_retry`: Provide clear, revised instructions for the Controller to formulate the new tool input for the retry. **Crucially, if the failure was due to the Executor describing its action or output instead of providing the direct content, these instructions MUST explicitly guide the Controller to instruct the Executor to: 'Your Final Answer for this step must be ONLY the [specific expected content, e.g., the synthesized report, the generated code], without any introductory phrases, self-reflection, or meta-commentary regarding your process.'**
        iii. `confidence_in_correction`: Your confidence (0.0-1.0) that this suggested retry approach will succeed.
Output ONLY a JSON object adhering to this schema:
{format_instructions}

Do not include any preamble or explanation outside the JSON object.
"""

class EvaluationResult(BaseModel):
    overall_success: bool = Field(description="Boolean indicating if the overall user query was successfully addressed by the executed plan.")
    confidence_score: float = Field(description="A score from 0.0 to 1.0 indicating confidence in the success/failure assessment.")
    assessment: str = Field(description="A concise, user-facing explanation of why the plan succeeded or failed overall, OR a brief confirmation if successful and final_answer_content is provided.")
    final_answer_content: Optional[str] = Field(
        default=None,
        description="If overall_success is True and the plan generated a direct user-facing answer (typically from the last step), this field MUST contain that exact, complete answer. Otherwise, it should be null or omitted."
    )
    suggestions_for_replan: Optional[List[str]] = Field(description="If the plan failed, a list of high-level suggestions for how a new plan might better achieve the goal. Null if successful.", default=None)

OVERALL_EVALUATOR_SYSTEM_PROMPT_TEMPLATE = """You are an expert Overall Plan Evaluator for a research agent.
Your task is to assess if the executed multi-step plan successfully achieved the user's original query, based on a summary of the plan's execution and the final answer produced by the last step of the plan.

**Context for Overall Evaluation:**
- Original User Query: {original_user_query}
- Summary of Executed Plan Steps & Outcomes:
  ---
  {executed_plan_summary}
  ---
- Output from the last successful plan step (this is the primary candidate for the user's final answer):
  ---
  {final_agent_answer}
  ---

**Your Evaluation Task:**
1.  `overall_success` (True/False): Did the agent, through the executed plan, fully and accurately address all aspects of the `original_user_query`?
    * Consider if the `final_agent_answer` (output from the last step) appropriately fulfills the user's request.
2.  `confidence_score` (0.0-1.0): Your confidence in this assessment.
3.  `final_answer_content`:
    * If `overall_success` is True AND the `final_agent_answer` (output from the last step) is the direct, complete, and appropriate answer for the user (e.g., it's the synthesized report, the generated text, not just a confirmation like "file written"), then this field MUST contain the **exact, verbatim content of `final_agent_answer`**.
    * If `overall_success` is True but the `final_agent_answer` is just a confirmation message (e.g., "File 'report.txt' written successfully to workspace.") and not the substantive content the user ultimately wanted to see, this `final_answer_content` field should be null or omitted.
    * If `overall_success` is False, this field MUST be null or omitted.
4.  `assessment`:
    * If `overall_success` is True and `final_answer_content` is populated (meaning the last step's output is the final answer), this `assessment` field should be a *very brief* confirmation message, like "The plan completed successfully and the information has been synthesized." or "All steps executed as planned. Here is the result:"
    * If `overall_success` is True but `final_answer_content` is null (e.g., the plan involved actions like writing files but no direct textual answer was expected from the last step for display), this `assessment` field should briefly summarize what was achieved (e.g., "The research was completed and files were generated as requested.").
    * If `overall_success` is False, this `assessment` field should be a concise, user-facing explanation of *why* the plan failed overall.
5.  `suggestions_for_replan` (Optional List[str]): If `overall_success` is False, provide a few high-level, actionable suggestions for how a *new* plan (if the user chooses to re-engage) might better achieve the original goal. These are not detailed steps, but strategic advice.

Output ONLY a JSON object adhering to this schema:
{format_instructions}

Do not include any preamble or explanation outside of the JSON object.
"""

async def evaluate_step_outcome_and_suggest_correction(
    original_user_query: str,
    plan_step_being_evaluated: PlanStep,
    controller_tool_used: Optional[str],
    controller_tool_input: Optional[str],
    step_executor_output: str,
    available_tools: List[BaseTool],
    session_data_entry: Dict[str, Any],
    callback_handler: Optional[BaseCallbackHandler] = None
) -> Optional[StepCorrectionOutcome]:
    logger.info(f"Evaluator (Step): Evaluating step '{plan_step_being_evaluated.step_id}: {plan_step_being_evaluated.description[:50]}...'")

    callbacks_for_invoke: List[BaseCallbackHandler] = []
    if callback_handler:
        callbacks_for_invoke.append(callback_handler)
        # Added critical debug log for consistency with other components
        logger.critical(f"CRITICAL_DEBUG: EVALUATOR_STEP - evaluate_step_outcome_and_suggest_correction received callback_handler: {type(callback_handler).__name__}")
    else:
        logger.critical("CRITICAL_DEBUG: EVALUATOR_STEP - evaluate_step_outcome_and_suggest_correction received NO callback_handler.")


    evaluator_llm_id_override = session_data_entry.get("session_evaluator_llm_id")
    evaluator_provider = settings.evaluator_provider
    evaluator_model_name = settings.evaluator_model_name
    if evaluator_llm_id_override:
        try:
            provider_override, model_override = evaluator_llm_id_override.split("::", 1)
            if provider_override in ["gemini", "ollama"] and model_override:
                evaluator_provider, evaluator_model_name = provider_override, model_override
                logger.info(f"Evaluator (Step): Using session override LLM: {evaluator_llm_id_override}")
            else:
                logger.warning(f"Evaluator (Step): Invalid session LLM ID structure '{evaluator_llm_id_override}'. Using system default.")
        except ValueError:
            logger.warning(f"Evaluator (Step): Invalid session LLM ID format '{evaluator_llm_id_override}'. Using system default.")

    try:
        # Added critical debug log for consistency
        logger.critical(f"CRITICAL_DEBUG: EVALUATOR_STEP - About to call get_llm. Callbacks_for_invoke: {[type(cb).__name__ for cb in callbacks_for_invoke] if callbacks_for_invoke else 'None'}")
        evaluator_llm: BaseChatModel = get_llm(
            settings,
            provider=evaluator_provider,
            model_name=evaluator_model_name,
            requested_for_role=LOG_SOURCE_EVALUATOR_STEP,
            callbacks=callbacks_for_invoke # *** MODIFIED: Pass callbacks_for_invoke to get_llm ***
        )
        logger.info(f"Evaluator (Step): Using LLM {evaluator_provider}::{evaluator_model_name}")
    except Exception as e:
        logger.error(f"Evaluator (Step): Failed to initialize LLM: {e}", exc_info=True)
        return None

    parser = JsonOutputParser(pydantic_object=StepCorrectionOutcome)
    format_instructions = parser.get_format_instructions()
    tools_summary_for_eval = "\n".join([f"- {tool.name}: {tool.description.split('.')[0]}" for tool in available_tools])

    prompt = ChatPromptTemplate.from_messages([
        ("system", STEP_EVALUATOR_SYSTEM_PROMPT_TEMPLATE),
        ("human", "Evaluate the step execution based on the provided context and output your assessment in the specified JSON format.")
    ])
    chain = prompt | evaluator_llm | parser

    try:
        evaluator_input_payload = {
            "original_user_query": original_user_query,
            "current_step_description": plan_step_being_evaluated.description,
            "current_step_expected_outcome": plan_step_being_evaluated.expected_outcome,
            "controller_tool_used": controller_tool_used or "None",
            "controller_tool_input": controller_tool_input or "None",
            "step_executor_output": step_executor_output,
            "available_tools_summary": tools_summary_for_eval,
            "format_instructions": format_instructions
        }
        logger.debug(f"Evaluator (Step {plan_step_being_evaluated.step_id}) INPUT - Expected Outcome: '{plan_step_being_evaluated.expected_outcome}'")
        logger.debug(f"Evaluator (Step {plan_step_being_evaluated.step_id}) INPUT - Actual Executor Output (first 300 chars): '{step_executor_output[:300]}...'")

        eval_result_dict = await chain.ainvoke(
            evaluator_input_payload,
            config=RunnableConfig(
                callbacks=callbacks_for_invoke, # These are for chain start/end, etc.
                metadata={"component_name": LOG_SOURCE_EVALUATOR_STEP}
            )
        )

        if isinstance(eval_result_dict, StepCorrectionOutcome):
            eval_outcome = eval_result_dict
        elif isinstance(eval_result_dict, dict):
            eval_outcome = StepCorrectionOutcome(**eval_result_dict)
        else:
            logger.error(f"Step Evaluator LLM call returned an unexpected type: {type(eval_result_dict)}. Content: {eval_result_dict}")
            raw_output_chain = prompt | evaluator_llm | StrOutputParser()
            raw_output = await raw_output_chain.ainvoke(
                evaluator_input_payload,
                config=RunnableConfig(
                    callbacks=callbacks_for_invoke,
                    metadata={"component_name": LOG_SOURCE_EVALUATOR_STEP + "_ERROR_HANDLER"}
                )
            )
            logger.error(f"Step Evaluator Raw LLM output on Pydantic error: {raw_output}")
            return None

        logger.info(f"Evaluator (Step {plan_step_being_evaluated.step_id}): Achieved Goal: {eval_outcome.step_achieved_goal}, Recoverable: {eval_outcome.is_recoverable_via_retry}, Assessment: {eval_outcome.assessment_of_step}")
        return eval_outcome
    except Exception as e:
        logger.error(f"Evaluator (Step): Error during step evaluation for step {plan_step_being_evaluated.step_id}: {e}", exc_info=True)
        try:
            input_payload_for_error = {
                "original_user_query": original_user_query,
                "current_step_description": plan_step_being_evaluated.description,
                "current_step_expected_outcome": plan_step_being_evaluated.expected_outcome,
                "controller_tool_used": controller_tool_used or "None",
                "controller_tool_input": controller_tool_input or "None",
                "step_executor_output": step_executor_output,
                "available_tools_summary": tools_summary_for_eval,
                "format_instructions": format_instructions
            }
            raw_output_chain = prompt | evaluator_llm | StrOutputParser()
            raw_output = await raw_output_chain.ainvoke(
                input_payload_for_error,
                config=RunnableConfig(
                    callbacks=callbacks_for_invoke,
                    metadata={"component_name": LOG_SOURCE_EVALUATOR_STEP + "_ERROR_HANDLER"}
                )
            )
            logger.error(f"Step Evaluator Raw LLM output on general error: {raw_output}")
        except Exception as raw_e:
            logger.error(f"Step Evaluator: Failed to get raw LLM output during general error handling: {raw_e}")
        return None


async def evaluate_plan_outcome(
    original_user_query: str,
    executed_plan_summary: str,
    final_agent_answer: str,
    session_data_entry: Dict[str, Any],
    callback_handler: Optional[BaseCallbackHandler] = None
) -> Optional[EvaluationResult]:
    logger.info(f"Evaluator (Overall Plan): Evaluating outcome for query: {original_user_query[:100]}...")
    logger.debug(f"Evaluator (Overall Plan): Received final_agent_answer (last step output) for evaluation (first 300 chars): {final_agent_answer[:300]}...")


    callbacks_for_invoke: List[BaseCallbackHandler] = []
    if callback_handler:
        callbacks_for_invoke.append(callback_handler)
        # Added critical debug log for consistency with other components
        logger.critical(f"CRITICAL_DEBUG: EVALUATOR_OVERALL - evaluate_plan_outcome received callback_handler: {type(callback_handler).__name__}")
    else:
        logger.critical("CRITICAL_DEBUG: EVALUATOR_OVERALL - evaluate_plan_outcome received NO callback_handler.")

    evaluator_llm_id_override = session_data_entry.get("session_evaluator_llm_id")
    evaluator_provider = settings.evaluator_provider
    evaluator_model_name = settings.evaluator_model_name
    if evaluator_llm_id_override:
        try:
            provider_override, model_override = evaluator_llm_id_override.split("::", 1)
            if provider_override in ["gemini", "ollama"] and model_override:
                evaluator_provider, evaluator_model_name = provider_override, model_override
                logger.info(f"Evaluator (Overall Plan): Using session override LLM: {evaluator_llm_id_override}")
            else:
                logger.warning(f"Evaluator (Overall Plan): Invalid session LLM ID structure '{evaluator_llm_id_override}'. Using system default.")
        except ValueError:
            logger.warning(f"Evaluator (Overall Plan): Invalid session LLM ID format '{evaluator_llm_id_override}'. Using system default.")

    try:
        # Added critical debug log for consistency
        logger.critical(f"CRITICAL_DEBUG: EVALUATOR_OVERALL - About to call get_llm. Callbacks_for_invoke: {[type(cb).__name__ for cb in callbacks_for_invoke] if callbacks_for_invoke else 'None'}")
        evaluator_llm: BaseChatModel = get_llm(
            settings,
            provider=evaluator_provider,
            model_name=evaluator_model_name,
            requested_for_role=LOG_SOURCE_EVALUATOR_OVERALL,
            callbacks=callbacks_for_invoke # *** MODIFIED: Pass callbacks_for_invoke to get_llm ***
        )
        logger.info(f"Evaluator (Overall Plan): Using LLM {evaluator_provider}::{evaluator_model_name}")
    except Exception as e:
        logger.error(f"Evaluator (Overall Plan): Failed to initialize LLM: {e}", exc_info=True)
        return None

    parser = JsonOutputParser(pydantic_object=EvaluationResult)
    format_instructions = parser.get_format_instructions()

    prompt = ChatPromptTemplate.from_messages([
        ("system", OVERALL_EVALUATOR_SYSTEM_PROMPT_TEMPLATE),
        ("human", "Evaluate the overall plan execution based on the provided context and output your assessment in the specified JSON format.")
    ])
    chain = prompt | evaluator_llm | parser

    try:
        payload_for_overall_eval = {
            "original_user_query": original_user_query,
            "executed_plan_summary": executed_plan_summary,
            "final_agent_answer": final_agent_answer,
            "format_instructions": format_instructions
        }
        eval_result_dict = await chain.ainvoke(
            payload_for_overall_eval,
            config=RunnableConfig(
                callbacks=callbacks_for_invoke, # These are for chain start/end, etc.
                metadata={"component_name": LOG_SOURCE_EVALUATOR_OVERALL}
            )
        )

        if isinstance(eval_result_dict, EvaluationResult):
            eval_result = eval_result_dict
        elif isinstance(eval_result_dict, dict):
            eval_result = EvaluationResult(**eval_result_dict)
        else:
            logger.error(f"Overall Plan Evaluator LLM call returned an unexpected type: {type(eval_result_dict)}. Content: {eval_result_dict}")
            raw_output_chain = prompt | evaluator_llm | StrOutputParser()
            raw_output = await raw_output_chain.ainvoke(
                payload_for_overall_eval,
                config=RunnableConfig(
                    callbacks=callbacks_for_invoke,
                    metadata={"component_name": LOG_SOURCE_EVALUATOR_OVERALL + "_ERROR_HANDLER"}
                )
            )
            logger.error(f"Overall Plan Evaluator Raw LLM output on Pydantic error: {raw_output}")
            return None

        logger.info(f"Evaluator (Overall Plan): Evaluation complete. Success: {eval_result.overall_success}, Confidence: {eval_result.confidence_score:.2f}, Assessment: '{eval_result.assessment}', Final Answer Content provided: {bool(eval_result.final_answer_content)}")
        return eval_result
    except Exception as e:
        logger.error(f"Evaluator (Overall Plan): Error during overall plan evaluation: {e}", exc_info=True)
        try:
            payload_for_overall_eval_error = {
                 "original_user_query": original_user_query,
                 "executed_plan_summary": executed_plan_summary,
                 "final_agent_answer": final_agent_answer,
                 "format_instructions": format_instructions
            }
            raw_output_chain = prompt | evaluator_llm | StrOutputParser()
            raw_output = await raw_output_chain.ainvoke(
                payload_for_overall_eval_error,
                config=RunnableConfig(
                    callbacks=callbacks_for_invoke,
                    metadata={"component_name": LOG_SOURCE_EVALUATOR_OVERALL + "_ERROR_HANDLER"}
                )
            )
            logger.error(f"Overall Plan Evaluator Raw LLM output on general error: {raw_output}")
        except Exception as raw_e:
            logger.error(f"Overall Plan Evaluator: Failed to get raw LLM output on general error: {raw_e}")
        return None

