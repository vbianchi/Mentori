# -----------------------------------------------------------------------------
# Mentor::i Backend Server (Phase 17 - Task API Endpoints)
#
# This version adds a full suite of RESTful API endpoints for managing tasks
# and their history, backed by our new SQLite database. This replaces the
# previous WebSocket-based task management and prepares for the removal of
# localStorage on the frontend.
#
# Key Architectural Changes:
# 1.  **Database Session Management**: A `get_db` context manager is added to
#     ensure that each API request gets a dedicated database session that is
#     properly closed, even if errors occur.
# 2.  **CRUD API for Tasks**:
#     - `GET /api/tasks`: Fetches all tasks from the database.
#     - `POST /api/tasks`: Creates a new task.
#     - `PUT /api/tasks/{task_id}`: Renames a specific task.
#     - `DELETE /api/tasks/{task_id}`: Deletes a task and its history.
# 3.  **History Endpoint**: `GET /api/tasks/{task_id}/history` retrieves the
#     full message history for a given task.
# 4.  **Updated WebSocket Handlers**: The `handle_task_create` and
#     `handle_task_delete` WebSocket handlers are updated to perform their
#     operations against the database, ensuring consistency.
# -----------------------------------------------------------------------------

import asyncio
import logging
import os
import json
import threading
import shutil
import mimetypes
import re
from http.server import BaseHTTPRequestHandler, HTTPServer
from urllib.parse import urlparse, parse_qs
from contextlib import contextmanager
from dotenv import load_dotenv
import websockets
from werkzeug.formparser import parse_form_data
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from sqlalchemy.orm import Session

# --- Local Imports ---
from .langgraph_agent import agent_graph
from .tools.file_system import _resolve_path
from .tools import get_available_tools
from .database import init_db, SessionLocal, Task, MessageHistory

# --- Configuration & Globals ---
load_dotenv()
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

RUNNING_AGENTS = {}
ACTIVE_CONNECTIONS = set()


# --- NEW: Database Session Context Manager ---
@contextmanager
def get_db():
    """Provides a transactional scope around a series of operations."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# --- Tool Template (Unchanged) ---
TOOL_TEMPLATE = """
# -----------------------------------------------------------------------------
# Custom Tool: {tool_name}
#
# This tool was dynamically generated by the Mentor::i Tool Forge.
# It uses a powerful LLM as an engine to execute tasks based on its description.
# Description: {tool_description}
# -----------------------------------------------------------------------------

import logging
import os
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import ChatOllama

logger = logging.getLogger(__name__)

# --- Input Schema ---
class {class_name}Input(BaseModel):
{args_schema}

# --- Tool Logic ---
def {function_name}({function_signature}) -> str:
    \"\"\"{tool_description}\"\"\"
    try:
        args_dict = {{ {function_args_dict} }}
        logger.info(f"Executing custom tool '{tool_name}' with arguments: {{args_dict}}")
        
        prompt = f\"\"\"
You are a specialized AI assistant. Your purpose is to execute a single, specific task based
on the instructions provided.
A user has created a tool with the following description:

**Tool Description:** "{tool_description}"

You must now execute this tool with the following input arguments:

**Arguments:**
{{args_dict}}

Based on the tool's description and the provided arguments, perform the task and return a concise, direct answer.
\"\"\"

        llm_id = os.getenv("EDITOR_LLM_ID", "gemini::gemini-1.5-pro-latest")
        provider, model_name = llm_id.split("::")
        logger.info(f"Custom tool '{tool_name}' is using LLM: {{llm_id}}")

        if provider == "gemini":
            llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=os.getenv("GOOGLE_API_KEY"))
        elif provider == "ollama":
            llm = ChatOllama(model=model_name, base_url=os.getenv("OLLAMA_BASE_URL"))
        else:
            return f"Error: Unsupported LLM provider '{{provider}}' specified in EDITOR_LLM_ID."

        response = llm.invoke(prompt)
        return response.content
        
    except Exception as e:
        logger.error(f"Error executing custom tool '{tool_name}': {{e}}", exc_info=True)
        return f"Error in tool '{tool_name}': {{e}}"

# --- Tool Definition ---
tool = StructuredTool.from_function(
    func={function_name},
    name="{tool_name}",
    description="{tool_description}",
    args_schema={class_name}Input
)
"""


# --- Helper Functions (Unchanged) ---
def _safe_delete_workspace(task_id: str):
    try:
        workspace_path = _resolve_path("/app/workspace", task_id)
        if not os.path.abspath(workspace_path).startswith(os.path.abspath("/app/workspace")): raise PermissionError("Security check failed.")
        if os.path.isdir(workspace_path):
            shutil.rmtree(workspace_path)
            logger.info(f"Task '{task_id}': Successfully deleted workspace directory.")
        else: logger.warning(f"Task '{task_id}': Workspace directory not found for deletion.")
    except Exception as e: logger.error(f"Task '{task_id}': Error deleting workspace: {e}", exc_info=True)

# --- HTTP File Server Class (MODIFIED) ---
class WorkspaceHTTPHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        parsed_path = urlparse(self.path)
        path_parts = parsed_path.path.strip('/').split('/')
        
        # --- NEW: API Routes for Tasks ---
        if path_parts[0] == 'api' and path_parts[1] == 'tasks':
            if len(path_parts) == 2:
                self._handle_get_tasks()
            elif len(path_parts) == 4 and path_parts[3] == 'history':
                self._handle_get_task_history(path_parts[2])
            else:
                self._send_json_response(404, {'error': 'Not Found'})
            return

        # Existing routes
        path = parsed_path.path.rstrip('/')
        if path == '/api/models': self._handle_get_models()
        elif path == '/api/tools': self._handle_get_tools()
        elif path == '/api/workspace/items': self._handle_get_workspace_items(parsed_path)
        elif path == '/file-content': self._handle_get_file_content(parsed_path)
        elif path == '/api/workspace/raw': self._handle_get_raw_file(parsed_path)
        else: self._send_json_response(404, {'error': f"Not Found: The path '{path}' does not match any known API routes."})
    
    def do_POST(self):
        parsed_path = urlparse(self.path)
        path_parts = parsed_path.path.strip('/').split('/')

        # --- NEW: API Route for Creating Tasks ---
        if path_parts[0] == 'api' and path_parts[1] == 'tasks' and len(path_parts) == 2:
            self._handle_create_task()
            return
            
        # Existing routes
        path = parsed_path.path.rstrip('/')
        if path == '/api/tools': self._handle_create_tool()
        elif path == '/api/workspace/folders': self._handle_create_folder()
        elif path == '/api/workspace/files': self._handle_create_file()
        elif path == '/upload': self._handle_file_upload()
        else: self._send_json_response(404, {'error': f"Not Found: The POST path '{path}' does not match any known API routes."})

    def do_DELETE(self):
        parsed_path = urlparse(self.path)
        path_parts = parsed_path.path.strip('/').split('/')
        
        # --- NEW: API Route for Deleting Tasks ---
        if path_parts[0] == 'api' and path_parts[1] == 'tasks' and len(path_parts) == 3:
            self._handle_delete_task(path_parts[2])
            return

        # Existing routes
        path = parsed_path.path.rstrip('/')
        if path == '/api/workspace/items': self._handle_delete_workspace_item(parsed_path)
        else: self._send_json_response(404, {'error': f"Not Found: The path '{path}' does not match any known DELETE routes."})
    
    def do_PUT(self):
        parsed_path = urlparse(self.path)
        path_parts = parsed_path.path.strip('/').split('/')

        # --- NEW: API Route for Renaming Tasks ---
        if path_parts[0] == 'api' and path_parts[1] == 'tasks' and len(path_parts) == 3:
            self._handle_rename_task(path_parts[2])
            return

        # Existing routes
        path = parsed_path.path.rstrip('/')
        if path == '/api/workspace/items': self._handle_rename_workspace_item()
        else: self._send_json_response(404, {'error': f"Not Found: The path '{path}' does not match any known PUT routes."})
    
    def do_OPTIONS(self):
        self.send_response(204)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS, PUT, DELETE')
        self.send_header('Access-Control-Allow-Headers', 'X-Requested-With, Content-Type')
        self.end_headers()

    def _send_json_response(self, status_code, data):
        self.send_response(status_code)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode('utf-8'))

    # --- NEW: Task API Handlers ---
    def _handle_get_tasks(self):
        with get_db() as db:
            tasks = db.query(Task).all()
            tasks_data = [{"id": task.id, "name": task.name} for task in tasks]
            self._send_json_response(200, tasks_data)

    def _handle_create_task(self):
        content_length = int(self.headers['Content-Length'])
        body = json.loads(self.rfile.read(content_length))
        task_id = body.get('id')
        task_name = body.get('name')
        if not task_id or not task_name:
            return self._send_json_response(400, {'error': 'Missing id or name'})
        
        with get_db() as db:
            db_task = Task(id=task_id, name=task_name)
            db.add(db_task)
            db.commit()
            db.refresh(db_task)
            # Also create workspace directory
            os.makedirs(f"/app/workspace/{task_id}", exist_ok=True)
            self._send_json_response(201, {"id": db_task.id, "name": db_task.name})

    def _handle_rename_task(self, task_id: str):
        content_length = int(self.headers['Content-Length'])
        body = json.loads(self.rfile.read(content_length))
        new_name = body.get('name')
        if not new_name:
            return self._send_json_response(400, {'error': 'Missing name'})
            
        with get_db() as db:
            db_task = db.query(Task).filter(Task.id == task_id).first()
            if not db_task:
                return self._send_json_response(404, {'error': 'Task not found'})
            db_task.name = new_name
            db.commit()
            self._send_json_response(200, {"id": db_task.id, "name": db_task.name})

    def _handle_delete_task(self, task_id: str):
        with get_db() as db:
            db_task = db.query(Task).filter(Task.id == task_id).first()
            if not db_task:
                return self._send_json_response(404, {'error': 'Task not found'})
            db.delete(db_task)
            db.commit()
        # Also delete workspace directory
        _safe_delete_workspace(task_id)
        self._send_json_response(200, {'message': 'Task deleted successfully'})

    def _handle_get_task_history(self, task_id: str):
        with get_db() as db:
            messages = db.query(MessageHistory).filter(MessageHistory.task_id == task_id).all()
            # The messages are stored as JSON strings, so we need to parse them back
            history_data = [json.loads(msg.message_json) for msg in messages]
            self._send_json_response(200, history_data)

    # --- Existing Handlers (Unchanged) ---
    def _handle_create_tool(self):
        try:
            content_length = int(self.headers['Content-Length'])
            if content_length == 0: return self._send_json_response(400, {'error': 'Request body is empty.'})
            body = json.loads(self.rfile.read(content_length))
            tool_name = body.get('name'); tool_description = body.get('description'); tool_args = body.get('arguments', [])
            if not tool_name or not tool_description: return self._send_json_response(400, {'error': "Request must include 'name' and 'description' for the tool."})
            safe_name = re.sub(r'\W|^(?=\d)', '', tool_name.lower().replace(' ', '_'))
            if not safe_name: safe_name = "custom_tool"
            class_name = safe_name.title().replace('_', ''); function_name = safe_name
            args_schema_lines = []; func_sig_parts = []; func_args_dict_parts = []
            type_mapping = {"string": "str", "number": "int", "boolean": "bool"}
            if not tool_args: args_schema_lines.append("    pass")
            else:
                for arg in tool_args:
                    arg_name = arg.get('name', '').strip()
                    if not arg_name: continue
                    arg_type = type_mapping.get(arg.get('type'), 'str')
                    arg_desc = arg.get('description', '').replace('"', '\\"')
                    args_schema_lines.append(f'    {arg_name}: {arg_type} = Field(description="{arg_desc}")')
                    func_sig_parts.append(f"{arg_name}: {arg_type}")
                    func_args_dict_parts.append(f"'{arg_name}': {arg_name}")
            tool_code = TOOL_TEMPLATE.format(tool_name=tool_name, tool_description=tool_description.replace('"', '\\"'), class_name=class_name, function_name=function_name, args_schema="\n".join(args_schema_lines), function_signature=", ".join(func_sig_parts), function_args_dict=", ".join(func_args_dict_parts))
            file_path = os.path.join("backend", "tools", f"custom_{safe_name}.py")
            with open(file_path, "w") as f: f.write(tool_code)
            logger.info(f"Successfully generated intelligent custom tool file: {file_path}")
            self._send_json_response(201, {'message': f"Tool '{tool_name}' was created successfully as '{os.path.basename(file_path)}'."})
        except json.JSONDecodeError: self._send_json_response(400, {'error': 'Invalid JSON in request body.'})
        except Exception as e: logger.error(f"Error creating tool: {e}", exc_info=True); self._send_json_response(500, {'error': str(e)})

    def _handle_get_models(self):
        logger.info("Parsing .env to serve available and default models.")
        available_models, model_ids = [], set()
        def parse_and_add_models(env_var, provider_prefix):
            models_str = os.getenv(env_var)
            if models_str:
                for model_name in models_str.split(','):
                    model_name = model_name.strip()
                    if model_name:
                        full_id = f"{provider_prefix}::{model_name}"
                        if full_id not in model_ids:
                            available_models.append({"id": full_id, "name": model_name})
                            model_ids.add(full_id)
        parse_and_add_models("GEMINI_AVAILABLE_MODELS", "gemini"); parse_and_add_models("OLLAMA_AVAILABLE_MODELS", "ollama")
        safe_fallback_id = "gemini::gemini-1.5-flash-latest"; safe_fallback_name = "gemini-1.5-flash-latest"
        if not available_models: available_models.append({"id": safe_fallback_id, "name": safe_fallback_name})
        global_default_llm = os.getenv("DEFAULT_LLM_ID", safe_fallback_id)
        default_models = {"ROUTER_LLM_ID": os.getenv("ROUTER_LLM_ID", global_default_llm), "CHIEF_ARCHITECT_LLM_ID": os.getenv("CHIEF_ARCHITECT_LLM_ID", global_default_llm), "SITE_FOREMAN_LLM_ID": os.getenv("SITE_FOREMAN_LLM_ID", global_default_llm), "PROJECT_SUPERVISOR_LLM_ID": os.getenv("PROJECT_SUPERVISOR_LLM_ID", global_default_llm), "EDITOR_LLM_ID": os.getenv("EDITOR_LLM_ID", "gemini::gemini-1.5-pro-latest")}
        self._send_json_response(200, {"available_models": available_models, "default_models": default_models})
    
    def _handle_get_tools(self):
        logger.info("Serving available tools list.")
        try:
            loaded_tools = get_available_tools()
            formatted_tools = [{"name": tool.name, "description": tool.description} for tool in loaded_tools]
            self._send_json_response(200, {"tools": formatted_tools})
        except Exception as e: logger.error(f"Failed to get available tools: {e}", exc_info=True); self._send_json_response(500, {"error": "Could not retrieve tools."})

    def _handle_get_workspace_items(self, parsed_path):
        query_components = parse_qs(parsed_path.query)
        subdir = query_components.get("path", [None])[0]
        if not subdir: return self._send_json_response(400, {"error": "Missing 'path' query parameter."})
        base_workspace, items = "/app/workspace", []
        try:
            full_path = _resolve_path(base_workspace, subdir)
            if not os.path.isdir(full_path): return self._send_json_response(404, {"error": f"Directory not found: '{subdir}'"})
            for item_name in os.listdir(full_path):
                item_path = os.path.join(full_path, item_name)
                item_type = 'directory' if os.path.isdir(item_path) else 'file'
                try: item_size = os.path.getsize(item_path) if item_type == 'file' else 0
                except OSError: item_size = 0
                items.append({"name": item_name, "type": item_type, "size": item_size})
            self._send_json_response(200, {"items": items})
        except Exception as e: logger.error(f"Error listing workspace items for path '{subdir}': {e}", exc_info=True); self._send_json_response(500, {"error": str(e)})

    def _handle_delete_workspace_item(self, parsed_path):
        query_components = parse_qs(parsed_path.query)
        item_path_str = query_components.get("path", [None])[0]
        if not item_path_str: return self._send_json_response(400, {"error": "Missing 'path' query parameter."})
        base_workspace = "/app/workspace"
        try:
            full_path = _resolve_path(base_workspace, item_path_str)
            if not os.path.exists(full_path): return self._send_json_response(404, {"error": f"Item not found: '{item_path_str}'"})
            if os.path.isdir(full_path): shutil.rmtree(full_path)
            else: os.remove(full_path)
            logger.info(f"Successfully deleted item: {full_path}")
            self._send_json_response(200, {"message": f"Successfully deleted item: '{item_path_str}'"})
        except Exception as e: logger.error(f"Error deleting item '{item_path_str}': {e}", exc_info=True); self._send_json_response(500, {"error": str(e)})

    def _handle_create_folder(self):
        try:
            content_length = int(self.headers['Content-Length']); body = json.loads(self.rfile.read(content_length))
            new_path_str = body.get('path')
            if not new_path_str: return self._send_json_response(400, {'error': "Request body must contain a 'path' key."})
            full_path = _resolve_path("/app/workspace", new_path_str)
            if os.path.exists(full_path): return self._send_json_response(409, {'error': f"Conflict: An item already exists at '{new_path_str}'."})
            os.makedirs(full_path); logger.info(f"Successfully created directory: {full_path}")
            self._send_json_response(201, {'message': f"Folder '{new_path_str}' created successfully."})
        except Exception as e: logger.error(f"Error creating folder: {e}", exc_info=True); self._send_json_response(500, {'error': str(e)})
            
    def _handle_create_file(self):
        try:
            content_length = int(self.headers['Content-Length']); body = json.loads(self.rfile.read(content_length))
            new_path_str = body.get('path')
            if not new_path_str: return self._send_json_response(400, {'error': "Request body must contain a 'path' key."})
            full_path = _resolve_path("/app/workspace", new_path_str)
            if os.path.exists(full_path): return self._send_json_response(409, {'error': f"Conflict: An item already exists at '{new_path_str}'."})
            with open(full_path, 'w') as f: pass
            logger.info(f"Successfully created empty file: {full_path}")
            self._send_json_response(201, {'message': f"File '{new_path_str}' created successfully."})
        except Exception as e: logger.error(f"Error creating file: {e}", exc_info=True); self._send_json_response(500, {'error': str(e)})

    def _handle_rename_workspace_item(self):
        try:
            content_length = int(self.headers['Content-Length']); body = json.loads(self.rfile.read(content_length))
            old_path_str, new_path_str = body.get('old_path'), body.get('new_path')
            if not old_path_str or not new_path_str: return self._send_json_response(400, {'error': "Request body must contain 'old_path' and 'new_path' keys."})
            base_workspace = "/app/workspace"; old_full_path = _resolve_path(base_workspace, old_path_str); new_full_path = _resolve_path(base_workspace, new_path_str)
            if not os.path.exists(old_full_path): return self._send_json_response(404, {'error': f"Source item not found: '{old_path_str}'."})
            if os.path.exists(new_full_path): return self._send_json_response(409, {'error': f"Destination already exists: '{new_path_str}'."})
            os.rename(old_full_path, new_full_path); logger.info(f"Successfully renamed '{old_full_path}' to '{new_full_path}'")
            self._send_json_response(200, {'message': f"Item renamed successfully to '{new_path_str}'."})
        except Exception as e: logger.error(f"Error renaming item: {e}", exc_info=True); self._send_json_response(500, {'error': str(e)})

    def _handle_get_file_content(self, parsed_path):
        query_components = parse_qs(parsed_path.query)
        workspace_id, filename = query_components.get("path", [None])[0], query_components.get("filename", [None])[0]
        if not workspace_id or not filename: return self._send_json_response(400, {"error": "Missing 'path' or 'filename' parameter."})
        try:
            full_path = _resolve_path(f"/app/workspace/{workspace_id}", filename)
            with open(full_path, 'r', encoding='utf-8') as f: content = f.read()
            self.send_response(200); self.send_header('Content-type', 'text/plain; charset=utf-8'); self.send_header('Access-Control-Allow-Origin', '*'); self.end_headers()
            self.wfile.write(content.encode('utf-8'))
        except Exception as e: self._send_json_response(500, {"error": f"Error reading file: {e}"})

    def _handle_get_raw_file(self, parsed_path):
        query_components = parse_qs(parsed_path.query)
        file_path_str = query_components.get("path", [None])[0]
        if not file_path_str: return self.send_error(400, "Missing 'path' query parameter.")
        try:
            full_path = _resolve_path("/app/workspace", file_path_str)
            if not os.path.isfile(full_path): return self.send_error(404, "File not found.")
            filename = os.path.basename(full_path); content_type, _ = mimetypes.guess_type(full_path)
            self.send_response(200); self.send_header('Content-type', content_type or 'application/octet-stream'); self.send_header('Access-Control-Allow-Origin', '*'); self.send_header('Content-Disposition', f'attachment; filename="{filename}"'); self.end_headers()
            with open(full_path, 'rb') as f: self.wfile.write(f.read())
        except Exception as e: logger.error(f"Error serving raw file '{file_path_str}': {e}", exc_info=True); self.send_error(500, "Internal Server Error")
    
    def _handle_file_upload(self):
        try:
            environ = {'wsgi.input': self.rfile, 'CONTENT_LENGTH': self.headers.get('Content-Length'), 'CONTENT_TYPE': self.headers.get('Content-Type'), 'REQUEST_METHOD': 'POST'}
            stream, form, files = parse_form_data(environ)
            workspace_id = form.get('workspace_id')
            if not workspace_id: return self._send_json_response(400, {'error': 'Missing workspace_id field.'})
            uploaded_files = files.getlist('file')
            if not uploaded_files: return self._send_json_response(400, {'error': 'No file(s) uploaded.'})
            for file_storage in uploaded_files:
                filename = file_storage.filename
                if not filename: continue
                full_path = _resolve_path(f"/app/workspace/{workspace_id}", os.path.basename(filename))
                file_storage.save(full_path)
                logger.info(f"Uploaded '{filename}' to workspace '{workspace_id}'")
            self._send_json_response(200, {'message': f"File(s) uploaded successfully."})
        except Exception as e: logger.error(f"File upload failed: {e}", exc_info=True); self._send_json_response(500, {'error': f'Server error during file upload: {e}'})


def run_http_server():
    httpd = HTTPServer((os.getenv("BACKEND_HOST", "0.0.0.0"), int(os.getenv("FILE_SERVER_PORT", 8766))), WorkspaceHTTPHandler)
    logger.info(f"Starting HTTP file server on port {httpd.server_port}")
    httpd.serve_forever()


# --- WebSocket Core Logic (MODIFIED) ---

NODE_NAME_MAPPING = { "initial_router_node": "Router", "summarize_history_node": "Summarizer", "Memory_Updater": "Memory Updater", "Chief_Architect": "Chief Architect", "Plan_Reviewer": "Plan Reviewer", "Site_Foreman": "Site Foreman", "Project_Supervisor": "Project Supervisor", "Correction_Planner": "Correction Planner", "Handyman": "Handyman", "Worker": "Worker", "Editor": "Editor" }
BROADCAST_NODES = set(NODE_NAME_MAPPING.keys())

async def broadcast_event(event):
    if ACTIVE_CONNECTIONS:
        message = json.dumps(event, default=str)
        tasks = [conn.send(message) for conn in ACTIVE_CONNECTIONS]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result in results:
            if isinstance(result, Exception): logger.warning(f"Failed to send message to a client: {result}")

async def agent_execution_wrapper(input_state, config):
    global RUNNING_AGENTS; task_id = config["configurable"]["thread_id"]
    try:
        logger.info(f"Task '{task_id}': Agent execution starting.")
        await broadcast_event({"type": "agent_started", "task_id": task_id})
        async for event in agent_graph.astream_events(input_state, config):
            event_type = event["event"]; node_name = event.get("name")
            if event_type in ["on_chain_start", "on_chain_end"] and node_name in BROADCAST_NODES:
                display_name = NODE_NAME_MAPPING.get(node_name, node_name)
                await broadcast_event({"type": "agent_event", "event": event_type, "name": display_name, "data": event['data'], "task_id": task_id})
            if event_type == "on_chain_end" and node_name == "Correction_Planner":
                new_plan = event.get("data", {}).get("output", {}).get("plan")
                if new_plan: logger.info(f"Task '{task_id}': Broadcasting plan update to frontend."); await broadcast_event({"type": "plan_updated", "plan": new_plan, "task_id": task_id})
        current_state = agent_graph.get_state(config)
        if current_state.next and "human_in_the_loop_node" in current_state.next:
            logger.info(f"Task '{task_id}': Paused for human approval.")
            await broadcast_event({"type": "plan_approval_request", "plan": current_state.values.get("plan"), "task_id": task_id})
        else:
            final_state = agent_graph.get_state(config)
            if final_state and (answer := final_state.values.get("answer")):
                final_answer_message = {"type": "final_answer", "data": answer, "task_id": task_id}
                if final_state.values.get("current_track") == "SIMPLE_TOOL_USE": final_answer_message["refresh_workspace"] = True
                await broadcast_event(final_answer_message)
            await broadcast_event({"type": "agent_stopped", "task_id": task_id, "message": "Agent run finished successfully."})
    except asyncio.CancelledError: logger.info(f"Task '{task_id}': Agent execution cancelled by user."); await broadcast_event({"type": "agent_stopped", "task_id": task_id, "message": "Agent run stopped by user."})
    except Exception as e: logger.error(f"Task '{task_id}': An error occurred during agent execution: {e}", exc_info=True); await broadcast_event({"type": "agent_stopped", "task_id": task_id, "message": f"An error occurred: {str(e)}"})
    finally:
        if task_id in RUNNING_AGENTS: del RUNNING_AGENTS[task_id]
        logger.info(f"Task '{task_id}': Cleaned up from RUNNING_AGENTS.")

async def run_agent_handler(data):
    global RUNNING_AGENTS; task_id = data.get("task_id"); prompt = data.get("prompt"); enabled_tools = data.get("enabled_tools")
    if not prompt or not task_id: return
    if task_id in RUNNING_AGENTS: logger.warning(f"Task '{task_id}': Agent is already running."); return await broadcast_event({"type": "error", "message": "Agent is already running for this task.", "task_id": task_id})
    try: recursion_limit = int(os.getenv("LANGGRAPH_RECURSION_LIMIT", 100))
    except (ValueError, TypeError): logger.warning("Invalid LANGGRAPH_RECURSION_LIMIT. Falling back to 100."); recursion_limit = 100
    config = {"recursion_limit": recursion_limit, "configurable": {"thread_id": task_id}}
    initial_state = {"messages": [HumanMessage(content=prompt)], "llm_config": data.get("llm_config", {}), "task_id": task_id, "enabled_tools": enabled_tools}
    logger.info(f"Task '{task_id}': Creating background task for new agent run with recursion limit of {recursion_limit}.")
    agent_task = asyncio.create_task(agent_execution_wrapper(initial_state, config))
    RUNNING_AGENTS[task_id] = agent_task

async def resume_agent_handler(data):
    global RUNNING_AGENTS; task_id = data.get("task_id"); feedback = data.get("feedback"); enabled_tools = data.get("enabled_tools")
    if not task_id or not feedback: return
    if task_id in RUNNING_AGENTS: logger.warning(f"Task '{task_id}': Agent is already running, cannot resume."); return
    try: recursion_limit = int(os.getenv("LANGGRAPH_RECURSION_LIMIT", 100))
    except (ValueError, TypeError): logger.warning("Invalid LANGGRAPH_RECURSION_LIMIT. Falling back to 100."); recursion_limit = 100
    config = {"recursion_limit": recursion_limit, "configurable": {"thread_id": task_id}}
    update_values = {"user_feedback": feedback, "enabled_tools": enabled_tools}
    if (plan := data.get("plan")) is not None: update_values["plan"] = plan
    agent_graph.update_state(config, update_values)
    logger.info(f"Task '{task_id}': Creating background task to resume agent execution with recursion limit of {recursion_limit}.")
    agent_task = asyncio.create_task(agent_execution_wrapper(None, config))
    RUNNING_AGENTS[task_id] = agent_task

async def handle_stop_agent(data):
    global RUNNING_AGENTS; task_id = data.get("task_id")
    if not task_id: return
    logger.info(f"Task '{task_id}': Received request to stop agent.")
    if task_id in RUNNING_AGENTS: RUNNING_AGENTS[task_id].cancel()
    else: logger.warning(f"Task '{task_id}': Stop requested, but no running agent found."); await broadcast_event({"type": "agent_stopped", "task_id": task_id, "message": "Agent was not running."})

# --- MODIFIED: These handlers now use the database ---
async def handle_task_create(data):
    task_id = data.get("task_id"); task_name = data.get("name")
    if not task_id or not task_name: return
    logger.info(f"Task '{task_id}': Received create task request via WebSocket.")
    with get_db() as db:
        if not db.query(Task).filter(Task.id == task_id).first():
            db_task = Task(id=task_id, name=task_name)
            db.add(db_task)
            db.commit()
            os.makedirs(f"/app/workspace/{task_id}", exist_ok=True)
            logger.info(f"Task '{task_id}': Created in database.")

async def handle_task_delete(data):
    global RUNNING_AGENTS; task_id = data.get("task_id")
    if not task_id: return
    logger.info(f"Task '{task_id}': Received delete task request via WebSocket.")
    if task_id in RUNNING_AGENTS: RUNNING_AGENTS[task_id].cancel()
    with get_db() as db:
        db_task = db.query(Task).filter(Task.id == task_id).first()
        if db_task:
            db.delete(db_task)
            db.commit()
            logger.info(f"Task '{task_id}': Deleted from database.")
    _safe_delete_workspace(task_id)

async def message_router(websocket):
    global ACTIVE_CONNECTIONS; ACTIVE_CONNECTIONS.add(websocket); client_id = id(websocket)
    logger.info(f"Client {client_id} connected. Total clients: {len(ACTIVE_CONNECTIONS)}")
    try:
        async for message in websocket:
            try:
                data = json.loads(message)
                message_type = data.get("type")
                handlers = {"run_agent": run_agent_handler, "resume_agent": resume_agent_handler, "stop_agent": handle_stop_agent, "task_create": handle_task_create, "task_delete": handle_task_delete}
                if message_type in handlers: await handlers[message_type](data)
                else: logger.warning(f"Received unknown message type: '{message_type}'")
            except Exception as e: logger.error(f"Error processing message: {e}", exc_info=True)
    except websockets.exceptions.ConnectionClosed as e: logger.info(f"Client {client_id} disconnected: {e.code}")
    finally: logger.info(f"Cleaning up for client {client_id}"); ACTIVE_CONNECTIONS.remove(websocket); logger.info(f"Client removed. Total clients: {len(ACTIVE_CONNECTIONS)}")

async def main():
    init_db()
    http_thread = threading.Thread(target=run_http_server, daemon=True)
    http_thread.start()
    async with websockets.serve(message_router, os.getenv("BACKEND_HOST", "0.0.0.0"), int(os.getenv("BACKEND_PORT", 8765))):
        logger.info("Mentor::i server is running.")
        await asyncio.Future()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Server shut down.")
