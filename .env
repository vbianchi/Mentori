# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables (.env.example)
#
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments.
# -----------------------------------------------------------------------------

# NCBI Entrez Email (for PubMed Tool - replace with your actual email)
# See: https://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen
ENTREZ_EMAIL="valerio.bianchi@wur.nl"

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIzaSyDrv4gI43qnRX1x3Mj8ugdnpAquV9T51BQ"

# Get your Tavily API Key from https://tavily.com (for Tavily Search Tool)
# Offers a free tier.
TAVILY_API_KEY="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"

# --- Available LLM Models for UI Selection ---
# Comma-separated list of Gemini models accessible with your API key.
# Ensure these models are available for your API key and region.
# Examples: gemini-1.5-flash,gemini-1.0-pro
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-05-06,gemini-2.0-flash,gemini-2.0-flash-lite,gemini-1.5-flash

# Comma-separated list of Ollama models you have pulled and want to make available.
# Ensure these models are running in your Ollama instance (ollama list).
# Examples: llama3.2:latest,mistral:latest,codellama:latest,gemma:7b-it
OLLAMA_AVAILABLE_MODELS=mistral:latest,llama3.2:latest,llama4:scout,deepseek-r1:8b,qwen3:4b,qwen3:8b,qwen3:30b-a3b,gemma3:12b,gemma3:latest

# Base URL for your Ollama instance (if used)
OLLAMA_BASE_URL=http://localhost:11434

# --- Core LLM Configuration ---
# Default LLM ID to be used if no role-specific LLM is set or for general UI selection.
# Format: "provider::model_name" (e.g., "gemini::gemini-1.5-flash", "ollama::llama3.2:latest")
DEFAULT_LLM_ID=gemini::gemini-2.0-flash-lite

# --- Role-Specific LLM Configuration (NEW for v2.1+) ---
# You can assign different LLMs to different agent components.
# If a role-specific LLM ID is not set or is invalid, the system will fall back to DEFAULT_LLM_ID for that role.
# Use the same "provider::model_name" format.

# For classifying user intent (PLAN vs. DIRECT_QA). Needs to be fast and accurate.
# Recommended: gemini::gemini-1.5-flash, ollama::mistral:7b-instruct
INTENT_CLASSIFIER_LLM_ID=gemini::gemini-1.5-flash

# For generating the initial multi-step plan. Needs strong reasoning and planning capabilities.
# Recommended: gemini::gemini-2.5-flash, ollama::llama3:70b-instruct
PLANNER_LLM_ID=gemini::gemini-2.5-flash-preview-05-20

# For validating each plan step and formulating precise tool inputs. Needs good instruction following and JSON output.
# Recommended: gemini::gemini-1.5-flash (with JSON mode if possible), ollama::codellama:13b-instruct
CONTROLLER_LLM_ID=gemini::gemini-2.0-flash

# Default LLM for the Executor (ReAct agent) if not overridden by UI selection for a specific task.
# This is the workhorse for executing individual plan steps.
# Recommended: gemini::gemini-1.5-flash, ollama::llama3:8b-instruct
EXECUTOR_DEFAULT_LLM_ID=gemini::gemini-2.0-flash-lite

# For evaluating the outcome of the executed plan. Needs strong analytical and reasoning skills.
# Recommended: gemini::gemini-2.5-flash, ollama::llama3:70b-instruct
EVALUATOR_LLM_ID=gemini::gemini-2.5-flash-preview-05-20

# --- Agent & LLM Tuning ---
# Max iterations and retries for the ReAct agent executor for each step.
AGENT_MAX_ITERATIONS=15
AGENT_MAX_STEP_RETRIES=1
# Conversation memory window size (number of past user/ai message pairs).
AGENT_MEMORY_WINDOW_K=10


# Default temperature for Gemini models (0.0 to 1.0). Higher is more creative.
GEMINI_TEMPERATURE=0.7
# Default temperature for Ollama models (0.0 to 1.0).
OLLAMA_TEMPERATURE=0.5


# --- Tool Specific Settings ---
# Max characters to read from a web page.
TOOL_WEB_READER_MAX_LENGTH=4000
# Timeout in seconds for web page reading.
TOOL_WEB_READER_TIMEOUT=15.0

# Timeout in seconds for shell command execution by the workspace_shell tool.
TOOL_SHELL_TIMEOUT=60
# Max characters of output to return from shell commands.
TOOL_SHELL_MAX_OUTPUT=3000

# Timeout in seconds for the python_package_installer tool.
TOOL_INSTALLER_TIMEOUT=300

# Default max results for PubMed searches.
TOOL_PUBMED_DEFAULT_MAX_RESULTS=5
# Max characters for PubMed abstract snippets.
TOOL_PUBMED_MAX_SNIPPET=250
# Max characters for PDF content reading before a warning is issued by the read_file tool.
TOOL_PDF_READER_WARNING_LENGTH=20000


# --- Server Settings ---
# Max WebSocket message size in bytes (e.g., 16MB = 16 * 1024 * 1024).
WEBSOCKET_MAX_SIZE_BYTES=16777216
# WebSocket ping interval in seconds.
WEBSOCKET_PING_INTERVAL=20
# WebSocket ping timeout in seconds.
WEBSOCKET_PING_TIMEOUT=30

# Timeout for direct shell commands initiated from UI (if any, distinct from agent tool).
DIRECT_COMMAND_TIMEOUT=120

# Hostname to use when constructing file URLs for the client (e.g., localhost, or your server's public IP/domain if exposed).
# For Docker, if backend is in a container and frontend is on host, 'localhost' usually works with network_mode: host.
# If backend is on a remote server, this should be the server's accessible hostname/IP.
FILE_SERVER_HOSTNAME=localhost


# --- Optional Settings ---
# Logging level for the backend (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO





# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables
#
# INSTRUCTIONS:
# 1. Copy this file to a new file named .env (cp .env.example .env)
# 2. Fill in the required values (e.g., GOOGLE_API_KEY).
# 3. The .env file is included in .gitignore to protect your secrets.
#
# This file serves as a complete blueprint for the project's configuration.
# -----------------------------------------------------------------------------

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIzaSyDrv4gI43qnRX1x3Mj8ugdnpAquV9T51BQ"

# Get your Tavily API Key from https://tavily.com (for the search tool)
TAVILY_API_KEY="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"

# Required for the PubMed search tool.
# See: https://www.ncbi.nlm.nih.gov/books/NBK25497/
ENTREZ_EMAIL="valerio.bianchi@wur.nl"


# --- Available LLM Models for UI Selection ---
# Comma-separated list of Gemini models accessible with your API key.
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-05-06,gemini-2.0-flash,gemini-2.0-flash-lite,gemini-1.5-flash

# Comma-separated list of Ollama models you have pulled locally.
# Ensure 'ollama serve' is running and you have pulled these models.
OLLAMA_AVAILABLE_MODELS=mistral:latest,llama3.2:latest,llama4:scout,deepseek-r1:8b,qwen3:4b,qwen3:8b,qwen3:30b-a3b,gemma3:12b,gemma3:latest

# Base URL for your local Ollama instance.
OLLAMA_BASE_URL=http://localhost:11434


# --- Core LLM Configuration ---
# Default LLM to use if a specific role's LLM is not set or fails.
# Format: "provider::model_name" (e.g., "gemini::gemini-1.5-flash", "ollama::llama3")
DEFAULT_LLM_ID=gemini::gemini-2.0-flash


# --- Role-Specific LLM Configuration (Development Defaults) ---
# For initial development (Phases 1-3), all roles are defaulted to a single, fast model.
# As we implement and tune each component in Phase 4, these can be updated to more
# specialized models as commented in the recommendations.

# For classifying user intent (PLAN vs. DIRECT_QA). Needs to be fast.
# Recommended: gemini::gemini-1.5-flash, ollama::mistral
INTENT_CLASSIFIER_LLM_ID=gemini::gemini-2.0-flash

# For generating the initial multi-step plan. Needs strong reasoning.
# Recommended: gemini::gemini-1.5-pro, ollama::llama3
PLANNER_LLM_ID=gemini::gemini-2.0-flash

# For validating each plan step and formulating tool inputs. Needs good instruction following.
# Recommended: gemini::gemini-1.5-flash, ollama::codellama
CONTROLLER_LLM_ID=gemini::gemini-2.0-flash

# Default workhorse LLM for the Executor (ReAct agent) if not overridden by the UI.
# Recommended: gemini::gemini-1.5-flash, ollama::llama3
EXECUTOR_LLM_ID=gemini::gemini-2.0-flash

# For evaluating the outcome of an executed plan. Needs strong analytical skills.
# Recommended: gemini::gemini-1.5-pro, ollama::llama3
EVALUATOR_LLM_ID=gemini::gemini-2.0-flash


# --- Agent & LLM Tuning ---
# Max iterations for the agent executor loop for a single step.
AGENT_MAX_ITERATIONS=15
# Conversation memory window size (number of past user/AI message pairs to keep).
AGENT_MEMORY_WINDOW_K=10

# Default model temperature (0.0 to 1.0). Higher is more creative.
GEMINI_TEMPERATURE=0.5
OLLAMA_TEMPERATURE=0.5


# --- Tool Specific Settings ---
# Timeout in seconds for shell commands (e.g., running python, Rscript, git).
TOOL_SHELL_TIMEOUT=120
# Max characters of output to return from a shell command.
TOOL_SHELL_MAX_OUTPUT=5000

# Timeout in seconds for installing packages (e.g., pip, uv).
TOOL_INSTALLER_TIMEOUT=300


# --- Server Settings ---
# The backend server will run on this port.
# Ensure this port is free or change it.
BACKEND_PORT=8765

# The file server for serving agent-created artifacts will run on this port.
FILE_SERVER_PORT=8766

# Hostname to use when constructing file URLs for the client.
# For Docker, 'localhost' works if ports are mapped to the host.
# If running on a remote server, this should be the server's public IP/domain.
FILE_SERVER_HOSTNAME=localhost

# Max WebSocket message size in bytes (e.g., 16MB = 16 * 1024 * 1024).
WEBSOCKET_MAX_SIZE_BYTES=16777216


# --- Optional Settings ---
# Logging level for the backend (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO