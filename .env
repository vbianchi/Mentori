# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables (.env.example)
#
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments.
# -----------------------------------------------------------------------------

# NCBI Entrez Email (for PubMed Tool - replace with your actual email)
# See: https://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen
ENTREZ_EMAIL="valerio.bianchi@wur.nl"

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIzaSyDrv4gI43qnRX1x3Mj8ugdnpAquV9T51BQ"

# Get your Tavily API Key from https://tavily.com (for Tavily Search Tool)
# Offers a free tier.
TAVILY_API_KEY="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"

# --- Available LLM Models for UI Selection ---
# Comma-separated list of Gemini models accessible with your API key.
# Ensure these models are available for your API key and region.
# Examples: gemini-1.5-flash,gemini-1.0-pro
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-05-06,gemini-2.0-flash,gemini-2.0-flash-lite,gemini-1.5-flash

# Comma-separated list of Ollama models you have pulled and want to make available.
# Ensure these models are running in your Ollama instance (ollama list).
# Examples: llama3.2:latest,mistral:latest,codellama:latest,gemma:7b-it
OLLAMA_AVAILABLE_MODELS=mistral:latest,llama3.2:latest,llama4:scout,deepseek-r1:8b,qwen3:4b,qwen3:8b,qwen3:30b-a3b,gemma3:12b,gemma3:latest

# Base URL for your Ollama instance (if used)
OLLAMA_BASE_URL=http://localhost:11434

# --- Core LLM Configuration ---
# Default LLM ID to be used if no role-specific LLM is set or for general UI selection.
# Format: "provider::model_name" (e.g., "gemini::gemini-1.5-flash", "ollama::llama3.2:latest")
DEFAULT_LLM_ID=gemini::gemini-2.0-flash-lite

# --- Role-Specific LLM Configuration (NEW for v2.1+) ---
# You can assign different LLMs to different agent components.
# If a role-specific LLM ID is not set or is invalid, the system will fall back to DEFAULT_LLM_ID for that role.
# Use the same "provider::model_name" format.

# For classifying user intent (PLAN vs. DIRECT_QA). Needs to be fast and accurate.
# Recommended: gemini::gemini-1.5-flash, ollama::mistral:7b-instruct
INTENT_CLASSIFIER_LLM_ID=gemini::gemini-1.5-flash

# For generating the initial multi-step plan. Needs strong reasoning and planning capabilities.
# Recommended: gemini::gemini-2.5-flash, ollama::llama3:70b-instruct
PLANNER_LLM_ID=gemini::gemini-2.5-flash-preview-05-20

# For validating each plan step and formulating precise tool inputs. Needs good instruction following and JSON output.
# Recommended: gemini::gemini-1.5-flash (with JSON mode if possible), ollama::codellama:13b-instruct
CONTROLLER_LLM_ID=gemini::gemini-2.0-flash

# Default LLM for the Executor (ReAct agent) if not overridden by UI selection for a specific task.
# This is the workhorse for executing individual plan steps.
# Recommended: gemini::gemini-1.5-flash, ollama::llama3:8b-instruct
EXECUTOR_DEFAULT_LLM_ID=gemini::gemini-2.0-flash-lite

# For evaluating the outcome of the executed plan. Needs strong analytical and reasoning skills.
# Recommended: gemini::gemini-2.5-flash, ollama::llama3:70b-instruct
EVALUATOR_LLM_ID=gemini::gemini-2.5-flash-preview-05-20

# --- Agent & LLM Tuning ---
# Max iterations and retries for the ReAct agent executor for each step.
AGENT_MAX_ITERATIONS=15
AGENT_MAX_STEP_RETRIES=1
# Conversation memory window size (number of past user/ai message pairs).
AGENT_MEMORY_WINDOW_K=10


# Default temperature for Gemini models (0.0 to 1.0). Higher is more creative.
GEMINI_TEMPERATURE=0.7
# Default temperature for Ollama models (0.0 to 1.0).
OLLAMA_TEMPERATURE=0.5


# --- Tool Specific Settings ---
# Max characters to read from a web page.
TOOL_WEB_READER_MAX_LENGTH=4000
# Timeout in seconds for web page reading.
TOOL_WEB_READER_TIMEOUT=15.0

# Timeout in seconds for shell command execution by the workspace_shell tool.
TOOL_SHELL_TIMEOUT=60
# Max characters of output to return from shell commands.
TOOL_SHELL_MAX_OUTPUT=3000

# Timeout in seconds for the python_package_installer tool.
TOOL_INSTALLER_TIMEOUT=300

# Default max results for PubMed searches.
TOOL_PUBMED_DEFAULT_MAX_RESULTS=5
# Max characters for PubMed abstract snippets.
TOOL_PUBMED_MAX_SNIPPET=250
# Max characters for PDF content reading before a warning is issued by the read_file tool.
TOOL_PDF_READER_WARNING_LENGTH=20000


# --- Server Settings ---
# Max WebSocket message size in bytes (e.g., 16MB = 16 * 1024 * 1024).
WEBSOCKET_MAX_SIZE_BYTES=16777216
# WebSocket ping interval in seconds.
WEBSOCKET_PING_INTERVAL=20
# WebSocket ping timeout in seconds.
WEBSOCKET_PING_TIMEOUT=30

# Timeout for direct shell commands initiated from UI (if any, distinct from agent tool).
DIRECT_COMMAND_TIMEOUT=120

# Hostname to use when constructing file URLs for the client (e.g., localhost, or your server's public IP/domain if exposed).
# For Docker, if backend is in a container and frontend is on host, 'localhost' usually works with network_mode: host.
# If backend is on a remote server, this should be the server's accessible hostname/IP.
FILE_SERVER_HOSTNAME=localhost


# --- Optional Settings ---
# Logging level for the backend (DEBUG, INFO, WARNING, ERROR, CRITICAL).
LOG_LEVEL=INFO

