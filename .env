# -----------------------------------------------------------------------------
# ResearchAgent Environment Variables
#
# INSTRUCTIONS:
# 1. Copy this file to a new file named .env (cp .env.example .env)
# 2. Fill in the required values (e.g., GOOGLE_API_KEY).
# 3. The .env file is included in .gitignore to protect your secrets.
#
# This file serves as a complete blueprint for the project's configuration.
# -----------------------------------------------------------------------------

# --- Required API Keys ---
# Get your Google API Key from Google AI Studio: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="AIzaSyDrv4gI43qnRX1x3Mj8ugdnpAquV9T51BQ"

# Get your Tavily API Key from https://tavily.com (for the search tool)
TAVILY_API_KEY="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"

# Required for the PubMed search tool.
# See: https://www.ncbi.nlm.nih.gov/books/NBK25497/
ENTREZ_EMAIL="valerio.bianchi@wur.nl"

# --- Model Configuration (Optional) ---
# This section defines the LLMs used by each agent role.
# The format is "provider::model_name".
# Supported providers: "gemini", "ollama".

# -- Available Models --
# Comma-separated list of models the UI should display in the dropdowns.
GEMINI_AVAILABLE_MODELS=gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-05-06,gemini-2.0-flash,gemini-2.0-flash-lite,gemini-1.5-flash
OLLAMA_AVAILABLE_MODELS=mistral:latest,llama3.2:latest,llama4:scout,deepseek-r1:8b,qwen3:4b,qwen3:8b,qwen3:30b-a3b,gemma3:12b,gemma3:latest

# -- Default Models for Each Agent Role --
# You can override the default for any agent by setting the specific variable.
# If a specific variable is not set, the UI will use the global DEFAULT_LLM_ID.
DEFAULT_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Router: Classifies the user's initial request.
ROUTER_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Librarian: Handles simple, direct questions.
LIBRARIAN_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Chief Architect: Creates the structured plan.
CHIEF_ARCHITECT_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Site Foreman: Prepares each tool call.
SITE_FOREMAN_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Worker: (Future use) Can be made intelligent. For now, this is not used by the backend.
WORKER_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Project Supervisor: Evaluates the outcome of each step.
PROJECT_SUPERVISOR_LLM_ID="gemini::gemini-2.0-flash-lite"

# The Editor: Synthesizes the final report for the user.
EDITOR_LLM_ID="gemini::gemini-2.0-flash-lite"


# --- Server Configuration (Optional) ---
# The host and port for the backend WebSocket server.
BACKEND_HOST="0.0.0.0"
BACKEND_PORT="8765"

# The host and port for the HTTP file server.
FILE_SERVER_PORT="8766"

# The base URL for a local Ollama server, if used.
OLLAMA_BASE_URL="http://host.docker.internal:11434"

# Set the logging level for the backend. (e.g., DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL="INFO"
